{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb8dfb1",
      "metadata": {
        "id": "8eb8dfb1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d43d6a",
      "metadata": {
        "id": "14d43d6a",
        "outputId": "d5ca74c4-3a6a-4a02-b9a1-bcd39b3b26e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/extended_stress_detection_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3369236451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/extended_stress_detection_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/extended_stress_detection_data.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/extended_stress_detection_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b03f9c",
      "metadata": {
        "id": "92b03f9c"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9116eeee",
      "metadata": {
        "id": "9116eeee"
      },
      "source": [
        "# Stress Detection\n",
        "\n",
        "## 1. Data Cleaning and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c47dca",
      "metadata": {
        "id": "52c47dca"
      },
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_curve, roc_auc_score)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9deb0693",
      "metadata": {
        "id": "9deb0693"
      },
      "outputs": [],
      "source": [
        "# Check basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Column Names and Types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Basic Statistics:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee7fad21",
      "metadata": {
        "id": "ee7fad21"
      },
      "outputs": [],
      "source": [
        "# Check target variable distribution\n",
        "print(\"Target Variable Distribution:\")\n",
        "print(df['Stress_Detection'].value_counts())\n",
        "print(\"\\nTarget Variable Percentage:\")\n",
        "print(df['Stress_Detection'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47368ce9",
      "metadata": {
        "id": "47368ce9"
      },
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "print(\"Handling missing values...\")\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "# Fill numerical missing values with median\n",
        "numerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
        "for col in numerical_cols:\n",
        "    if df_cleaned[col].isnull().sum() > 0:\n",
        "        df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
        "\n",
        "# Fill categorical missing values with mode\n",
        "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    if df_cleaned[col].isnull().sum() > 0:\n",
        "        df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"Missing values after cleaning:\")\n",
        "print(df_cleaned.isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d0c993",
      "metadata": {
        "id": "42d0c993"
      },
      "outputs": [],
      "source": [
        "# Handle outliers using IQR method for key numerical features\n",
        "def remove_outliers_iqr(df, column, multiplier=1.5):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - multiplier * IQR\n",
        "    upper_bound = Q3 + multiplier * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Check for negative values in Social_Interactions (noticed in the data)\n",
        "print(\"Negative values in Social_Interactions:\", (df_cleaned['Social_Interactions'] < 0).sum())\n",
        "df_cleaned = df_cleaned[df_cleaned['Social_Interactions'] >= 0]\n",
        "\n",
        "print(f\"\\nDataset shape after cleaning: {df_cleaned.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b75ef55",
      "metadata": {
        "id": "0b75ef55"
      },
      "source": [
        "## 2. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599f7348",
      "metadata": {
        "id": "599f7348"
      },
      "outputs": [],
      "source": [
        "# Visualize target variable distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "df_cleaned['Stress_Detection'].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#f39c12', '#e74c3c'])\n",
        "axes[0].set_title('Stress Detection Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Stress Level')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].tick_params(rotation=0)\n",
        "\n",
        "# Pie chart\n",
        "df_cleaned['Stress_Detection'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%',\n",
        "                                                     colors=['#2ecc71', '#f39c12', '#e74c3c'])\n",
        "axes[1].set_title('Stress Detection Percentage', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078dac25",
      "metadata": {
        "id": "078dac25"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap for numerical features\n",
        "numerical_features = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "correlation_matrix = df_cleaned[numerical_features].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix of Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b10b3739",
      "metadata": {
        "id": "b10b3739"
      },
      "outputs": [],
      "source": [
        "# Categorical features analysis\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "categorical_features = ['Gender', 'Marital_Status', 'Smoking_Habit', 'Meditation_Practice']\n",
        "\n",
        "for idx, feature in enumerate(categorical_features):\n",
        "    ax = axes[idx//2, idx%2]\n",
        "    pd.crosstab(df_cleaned[feature], df_cleaned['Stress_Detection']).plot(\n",
        "        kind='bar', ax=ax, color=['#2ecc71', '#f39c12', '#e74c3c']\n",
        "    )\n",
        "    ax.set_title(f'{feature} vs Stress Detection', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel(feature)\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.legend(title='Stress Level')\n",
        "    ax.tick_params(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d874ca",
      "metadata": {
        "id": "57d874ca"
      },
      "source": [
        "## 3. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4d901a",
      "metadata": {
        "id": "7f4d901a"
      },
      "outputs": [],
      "source": [
        "# Create a copy for feature engineering\n",
        "df_fe = df_cleaned.copy()\n",
        "\n",
        "# 1. Create new features\n",
        "# Sleep efficiency (Sleep Quality / Sleep Duration)\n",
        "df_fe['Sleep_Efficiency'] = df_fe['Sleep_Quality'] / (df_fe['Sleep_Duration'] + 1e-5)\n",
        "\n",
        "# Work-Life balance indicator\n",
        "df_fe['Work_Life_Balance'] = df_fe['Work_Hours'] / (df_fe['Physical_Activity'] + df_fe['Social_Interactions'] + 1)\n",
        "\n",
        "# Health score (combination of health metrics)\n",
        "df_fe['Health_Score'] = (df_fe['Blood_Pressure'] + df_fe['Cholesterol_Level'] + df_fe['Blood_Sugar_Level']) / 3\n",
        "\n",
        "# Screen time to physical activity ratio\n",
        "df_fe['Screen_Activity_Ratio'] = df_fe['Screen_Time'] / (df_fe['Physical_Activity'] + 1)\n",
        "\n",
        "# Age groups\n",
        "df_fe['Age_Group'] = pd.cut(df_fe['Age'], bins=[0, 30, 40, 50, 100], labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
        "\n",
        "print(\"New features created!\")\n",
        "print(f\"Total features now: {df_fe.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bb0bf18",
      "metadata": {
        "id": "1bb0bf18"
      },
      "outputs": [],
      "source": [
        "# Convert time features to numeric (extract hour)\n",
        "def extract_hour(time_str):\n",
        "    try:\n",
        "        time_part = time_str.split()[0]\n",
        "        hour = int(time_part.split(':')[0])\n",
        "        if 'PM' in time_str and hour != 12:\n",
        "            hour += 12\n",
        "        elif 'AM' in time_str and hour == 12:\n",
        "            hour = 0\n",
        "        return hour\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "df_fe['Wake_Up_Hour'] = df_fe['Wake_Up_Time'].apply(extract_hour)\n",
        "df_fe['Bed_Time_Hour'] = df_fe['Bed_Time'].apply(extract_hour)\n",
        "\n",
        "# Drop original time columns\n",
        "df_fe = df_fe.drop(['Wake_Up_Time', 'Bed_Time'], axis=1)\n",
        "\n",
        "print(\"Time features converted to numeric!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6efb32",
      "metadata": {
        "id": "aa6efb32"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "label_encoders = {}\n",
        "categorical_columns = df_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "categorical_columns.remove('Stress_Detection')  # Don't encode target yet\n",
        "\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_fe[col] = le.fit_transform(df_fe[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "print(\"Categorical features encoded!\")\n",
        "print(f\"Encoded columns: {categorical_columns}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876f9599",
      "metadata": {
        "id": "876f9599"
      },
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df_fe.drop('Stress_Detection', axis=1)\n",
        "y = df_fe['Stress_Detection']\n",
        "\n",
        "# Encode target variable\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(y)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y_encoded.shape}\")\n",
        "print(f\"\\nTarget mapping: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f68ddd",
      "metadata": {
        "id": "d9f68ddd"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set size: {X_train_scaled.shape}\")\n",
        "print(f\"Test set size: {X_test_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f482b2a",
      "metadata": {
        "id": "7f482b2a"
      },
      "source": [
        "## 4. Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1862db5",
      "metadata": {
        "id": "f1862db5"
      },
      "outputs": [],
      "source": [
        "# Define multiple classification models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "}\n",
        "\n",
        "print(\"Models initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496d8534",
      "metadata": {
        "id": "496d8534"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate all models\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1\n",
        "    })\n",
        "\n",
        "    print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ad0964",
      "metadata": {
        "id": "94ad0964"
      },
      "outputs": [],
      "source": [
        "# Visualize model performance comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx//2, idx%2]\n",
        "    results_df.plot(x='Model', y=metric, kind='bar', ax=ax, color=colors[idx], legend=False)\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Model', fontsize=12)\n",
        "    ax.set_ylabel(metric, fontsize=12)\n",
        "    ax.set_ylim([0, 1.1])\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.3f', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067b9880",
      "metadata": {
        "id": "067b9880"
      },
      "outputs": [],
      "source": [
        "# Detailed evaluation of the best model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"DETAILED EVALUATION OF BEST MODEL ON BASELINE MODEL BEFORE HYPERPARAMETER TUNING:\\n {best_model_name}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "y_pred_best = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_best, target_names=le_target.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b25db8f",
      "metadata": {
        "id": "6b25db8f"
      },
      "outputs": [],
      "source": [
        "# Confusion matrices for all models\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (name, model) in enumerate(models.items()):\n",
        "    if idx < len(axes):\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', ax=axes[idx],\n",
        "                   xticklabels=le_target.classes_, yticklabels=le_target.classes_,\n",
        "                   cbar=False)\n",
        "        axes[idx].set_title(f'{name}', fontsize=11, fontweight='bold')\n",
        "        axes[idx].set_xlabel('Predicted')\n",
        "        axes[idx].set_ylabel('Actual')\n",
        "\n",
        "# Hide the last empty subplot\n",
        "if len(models) < len(axes):\n",
        "    fig.delaxes(axes[-1])\n",
        "\n",
        "plt.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "056ae075",
      "metadata": {
        "id": "056ae075"
      },
      "outputs": [],
      "source": [
        "# Feature importance (for tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=feature_importance, x='Importance', y='Feature', palette='viridis')\n",
        "    plt.title(f'Top 15 Feature Importances - {best_model_name}', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.ylabel('Feature', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(feature_importance.head(10).to_string(index=False))\n",
        "else:\n",
        "    print(f\"\\nFeature importance not available for {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05b41cf",
      "metadata": {
        "id": "a05b41cf"
      },
      "outputs": [],
      "source": [
        "# Cross-validation scores for all models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CROSS-VALIDATION SCORES (5-Fold)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cv_results = []\n",
        "for name, model in models.items():\n",
        "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    cv_results.append({\n",
        "        'Model': name,\n",
        "        'Mean CV Score': cv_scores.mean(),\n",
        "        'Std CV Score': cv_scores.std()\n",
        "    })\n",
        "    print(f\"{name}: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "cv_results_df = pd.DataFrame(cv_results).sort_values('Mean CV Score', ascending=False)\n",
        "print(\"\\n\" + cv_results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d086feeb",
      "metadata": {
        "id": "d086feeb"
      },
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "plt.figure(figsize=(14, 6))\n",
        "x_pos = np.arange(len(cv_results_df))\n",
        "plt.bar(x_pos, cv_results_df['Mean CV Score'], yerr=cv_results_df['Std CV Score'],\n",
        "        color='steelblue', alpha=0.8, capsize=5, edgecolor='black')\n",
        "plt.xlabel('Models', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Cross-Validation Accuracy', fontsize=12, fontweight='bold')\n",
        "plt.title('5-Fold Cross-Validation Scores for All Models', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xticks(x_pos, cv_results_df['Model'], rotation=45, ha='right')\n",
        "plt.ylim([0, 1.1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(cv_results_df['Mean CV Score']):\n",
        "    plt.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "465e5156",
      "metadata": {
        "id": "465e5156"
      },
      "source": [
        "## 5. Hyperparameter Tuning with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd1b8f9",
      "metadata": {
        "id": "efd1b8f9"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning with GridSearchCV and StratifiedKFold for KNN, Random Forest, and Logistic Regression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "# Define StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" \"*20 + \"ðŸ”§ HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1) K-Nearest Neighbors (KNN) Tuning\n",
        "print(\"\\n1ï¸âƒ£  Tuning K-Nearest Neighbors...\")\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid_knn = GridSearchCV(\n",
        "    estimator=KNeighborsClassifier(),\n",
        "    param_grid=param_grid_knn,\n",
        "    scoring='accuracy',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid_knn.fit(X_train_scaled, y_train)\n",
        "y_pred_knn_tuned = grid_knn.best_estimator_.predict(X_test_scaled)\n",
        "acc_knn_tuned = accuracy_score(y_test, y_pred_knn_tuned)\n",
        "\n",
        "print(f\"   Best params: {grid_knn.best_params_}\")\n",
        "print(f\"   Best CV accuracy: {grid_knn.best_score_:.4f}\")\n",
        "print(f\"   Test accuracy: {acc_knn_tuned:.4f}\")\n",
        "\n",
        "# 2) Random Forest Tuning\n",
        "print(\"\\n2ï¸âƒ£  Tuning Random Forest...\")\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt'],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid_rf,\n",
        "    scoring='accuracy',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid_rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf_tuned = grid_rf.best_estimator_.predict(X_test_scaled)\n",
        "acc_rf_tuned = accuracy_score(y_test, y_pred_rf_tuned)\n",
        "\n",
        "print(f\"   Best params: {grid_rf.best_params_}\")\n",
        "print(f\"   Best CV accuracy: {grid_rf.best_score_:.4f}\")\n",
        "print(f\"   Test accuracy: {acc_rf_tuned:.4f}\")\n",
        "\n",
        "# 3) Logistic Regression Tuning\n",
        "print(\"\\n3ï¸âƒ£  Tuning Logistic Regression...\")\n",
        "param_grid_lr = {\n",
        "    'penalty': ['l2'],\n",
        "    'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "    'solver': ['lbfgs', 'saga'],\n",
        "    'max_iter': [1000, 2000],\n",
        "    'class_weight': ['balanced', None]\n",
        "}\n",
        "\n",
        "grid_lr = GridSearchCV(\n",
        "    estimator=LogisticRegression(random_state=42),\n",
        "    param_grid=param_grid_lr,\n",
        "    scoring='accuracy',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid_lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr_tuned = grid_lr.best_estimator_.predict(X_test_scaled)\n",
        "acc_lr_tuned = accuracy_score(y_test, y_pred_lr_tuned)\n",
        "\n",
        "print(f\"   Best params: {grid_lr.best_params_}\")\n",
        "print(f\"   Best CV accuracy: {grid_lr.best_score_:.4f}\")\n",
        "print(f\"   Test accuracy: {acc_lr_tuned:.4f}\")\n",
        "\n",
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" \"*15 + \"ðŸ“Š TUNED MODELS PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "tuned_results = pd.DataFrame({\n",
        "    'Model': ['K-Nearest Neighbors', 'Random Forest', 'Logistic Regression'],\n",
        "    'Best CV Accuracy': [grid_knn.best_score_, grid_rf.best_score_, grid_lr.best_score_],\n",
        "    'Test Accuracy': [acc_knn_tuned, acc_rf_tuned, acc_lr_tuned]\n",
        "})\n",
        "\n",
        "print(tuned_results.to_string(index=False))\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0411eb71",
      "metadata": {
        "id": "0411eb71"
      },
      "source": [
        "## 6. Final Summary and Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315b688c",
      "metadata": {
        "id": "315b688c"
      },
      "outputs": [],
      "source": [
        "# Visualizations for tuned models\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 1) Bar chart comparing CV vs Test accuracy\n",
        "ax1 = axes[0]\n",
        "x = np.arange(len(tuned_results))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, tuned_results['Best CV Accuracy'], width, label='CV Accuracy', color='steelblue', edgecolor='black')\n",
        "bars2 = ax1.bar(x + width/2, tuned_results['Test Accuracy'], width, label='Test Accuracy', color='coral', edgecolor='black')\n",
        "\n",
        "ax1.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Tuned Models: CV vs Test Accuracy', fontsize=14, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(tuned_results['Model'], rotation=15, ha='right')\n",
        "ax1.legend()\n",
        "ax1.set_ylim([0, 1.1])\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 2) Confusion matrices for tuned models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create subplots for confusion matrices\n",
        "fig2, axes2 = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "tuned_preds = [y_pred_knn_tuned, y_pred_rf_tuned, y_pred_lr_tuned]\n",
        "tuned_names = ['KNN (Tuned)', 'Random Forest (Tuned)', 'Logistic Regression (Tuned)']\n",
        "\n",
        "for idx, (name, y_pred) in enumerate(zip(tuned_names, tuned_preds)):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes2[idx],\n",
        "                xticklabels=le_target.classes_, yticklabels=le_target.classes_,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    axes2[idx].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
        "    axes2[idx].set_xlabel('Predicted')\n",
        "    axes2[idx].set_ylabel('Actual')\n",
        "\n",
        "plt.suptitle('Confusion Matrices - Tuned Models', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) Classification reports for tuned models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" \"*20 + \"ðŸ“‹ DETAILED CLASSIFICATION REPORTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for name, y_pred in zip(tuned_names, tuned_preds):\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "109b0c2e",
      "metadata": {
        "id": "109b0c2e"
      },
      "outputs": [],
      "source": [
        "# Identify and display the BEST TUNED MODEL\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" \"*25 + \"ðŸ† BEST MODEL SELECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Sort by test accuracy to find the best model\n",
        "tuned_results_sorted = tuned_results.sort_values('Test Accuracy', ascending=False)\n",
        "best_tuned_model_name = tuned_results_sorted.iloc[0]['Model']\n",
        "best_cv_acc = tuned_results_sorted.iloc[0]['Best CV Accuracy']\n",
        "best_test_acc = tuned_results_sorted.iloc[0]['Test Accuracy']\n",
        "\n",
        "print(f\"\\nðŸ¥‡ WINNER: {best_tuned_model_name}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"   âœ“ Best CV Accuracy:   {best_cv_acc:.4f}\")\n",
        "print(f\"   âœ“ Test Accuracy:      {best_test_acc:.4f}\")\n",
        "print(f\"   âœ“ Improvement over CV: {(best_test_acc - best_cv_acc):.4f}\")\n",
        "\n",
        "# Get the best model parameters\n",
        "if best_tuned_model_name == 'K-Nearest Neighbors':\n",
        "    best_model = grid_knn.best_estimator_\n",
        "    best_params = grid_knn.best_params_\n",
        "    y_pred_best = y_pred_knn_tuned\n",
        "elif best_tuned_model_name == 'Random Forest':\n",
        "    best_model = grid_rf.best_estimator_\n",
        "    best_params = grid_rf.best_params_\n",
        "    y_pred_best = y_pred_rf_tuned\n",
        "else:  # Logistic Regression\n",
        "    best_model = grid_lr.best_estimator_\n",
        "    best_params = grid_lr.best_params_\n",
        "    y_pred_best = y_pred_lr_tuned\n",
        "\n",
        "print(f\"\\nðŸ“‹ Best Hyperparameters:\")\n",
        "print(\"-\"*80)\n",
        "for param, value in best_params.items():\n",
        "    print(f\"   â€¢ {param}: {value}\")\n",
        "\n",
        "# Detailed metrics for the best model\n",
        "precision = precision_score(y_test, y_pred_best, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred_best, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred_best, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nðŸ“Š Comprehensive Metrics for {best_tuned_model_name}:\")\n",
        "print(\"-\"*80)\n",
        "print(f\"   â€¢ Accuracy:  {best_test_acc:.4f}\")\n",
        "print(f\"   â€¢ Precision: {precision:.4f}\")\n",
        "print(f\"   â€¢ Recall:    {recall:.4f}\")\n",
        "print(f\"   â€¢ F1-Score:  {f1:.4f}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Confusion matrix for the best model\n",
        "print(f\"\\nðŸ“ˆ Confusion Matrix for Best Model: {best_tuned_model_name}\")\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=le_target.classes_, yticklabels=le_target.classes_,\n",
        "            cbar_kws={'label': 'Count'}, linewidths=1, linecolor='black')\n",
        "plt.title(f'ðŸ† Best Tuned Model: {best_tuned_model_name}\\nTest Accuracy: {best_test_acc:.4f}',\n",
        "          fontsize=14, fontweight='bold', pad=15)\n",
        "plt.xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\nðŸ“‹ Detailed Classification Report for Best Model ({best_tuned_model_name}):\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(y_test, y_pred_best, target_names=le_target.classes_))\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}